---
params:
  projectDir: "C:/Users/jbailard/Documents/R/mojn-stlk-drr"   # You will have to update this directory to match your project directory before knitting.
  reportNumber: "REPORT NUMBER"                           # Optional. Only include this if publishing in the semi-official Data Release Report Series. Contact Joe if you are.
  reportRefID: 123456                                     # This should match the Data Store Reference ID for this report.
  packageAbstract: >-
    This report summarizes data quality evaluations on discrete data collected for the Mojave Desert Network Inventory and Monitoring Program (MOJN I&M) Streams and Lakes (STLK) monitoring protocol from 2009 to 2019. Data collected includes lake level surveys, water quality measurements, water chemistry samples, and benthic macroinvertebrate assemblages.
  dataPackage1RefID: 2272463                              # Data Store reference ID for data set associated with this report. You must have at least one.
  dataPackage1Title: "Dataset 1 FULL TITLE"               # Should match title in data store.
  dataPackage1Description: "SHORT TITLE FOR DATASET 1"  
  dataPackage2RefID: 2272464                              # Data Store reference ID for data set associated with this report. You must have at least one.
  dataPackage2Title: "Dataset 2 FULL TITLE"               # Should match title in data store.
  dataPackage2Description: "SHORT TITLE FOR DATASET 1"  
      
title: "Mojave Desert Network Streams and Lakes Protocol 2009-2019"
subtitle: |
  | Data Release Report `r params$reportNumber` 
author:
  - name: "Author 1"                                      # Add or remove authors as appropriate.
    affiliation: |
      | Mojave Desert Network
      | Inventory and Monitoring Program 
      | 101 Katzenbach Drive
      | Boulder City, Nevada
date: "`r format(Sys.time(), '%d %B, %Y')`"
abstract: "`r params$packageAbstract`"
editor_options:
  chunk_output_type: inline
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl
link-citations: yes
output:
  html_document:
    df_print: kable
    fig_caption: true
    dev: svg
    highlight: haddock
    keep_md: yes
    smart: no
    theme: journal
    css: "common/journalnps.min.css"
    toc: yes
    toc_float: true
    number_sections: true
    includes:
        before_body:
          - "common/header.html"
        after_body: 
          - "common/footer.html"
  word_document:
    df_print: kable
    fig_caption: yes
    fig_height: 5
    fig_width: 5
    highlight: haddock
    reference_docx: "common/DRR Word Template.docx"
---

```{r setup, include=FALSE}

# This setup code loads both reproducible reporting packages
# (delete those not needed) and packages for the actual project.
# Note that it also generates the start of a BibTex literature cited
# including the citations for R and all used packages

# reproducible reporting packages
RRpackages <- c('markdown',     # links to Sundown rendering library
                'rmarkdown',    # newer rendering via pandoc
                'pander',       # alternative renderer for markdown,
                                # plus better tables than just knitr
                'knitr',
                "dataMaid",     # for makeCodebooks
                "R.rsp",        # dynamic generation of scientific reports
                "kimisc",       #
                "papeR",        # stat tables
                "texreg",       # formatting regression results for LaTeX
                                # or html
                "rmdHelpers",   # misc from Mark Peterson
                                #  thisFileName() thisFile_knit()
                'yaml',         # format data into markdown
                'rmdformats',   # templates including automatic ToC,
                                # also use_bookdown()
                'htmltools',    #
                "bibtex",
                "RefManageR",   # BibTeX reference manager
                "knitcitations" #
                )

inst <- RRpackages %in% installed.packages()
if (length(RRpackages[!inst]) > 0) {
   install.packages(RRpackages[!inst], dep = TRUE, repos = "https://cloud.r-project.org")
}
lapply(RRpackages, library, character.only = TRUE)

# Now repeat for packages used in the analyses
pkgList <- c("devtools",        # tends to be needed/useful
             "RODBC",           # for connection to a database. 
             "EML",             # for data package creation and validation
             "kableExtra",      # added features for table formatting. 
             "english",         # converts numbers into english. Good for all that English stuff.
             "remotes",         # for install_github()
             "tidyverse",       # useful
             "magrittr",
             "plotly",
             "svglite",
             "streamsandlakes")

inst <- pkgList %in% installed.packages()
if (length(pkgList[!inst]) > 0) {
   install.packages(pkgList[!inst], dep = TRUE, 
                    repos = "https://cloud.r-project.org")
}

lapply(pkgList, library, character.only = TRUE, quietly = TRUE)

if (! "EMLassemblyline" %in% installed.packages()) remotes::install_github("EDIorg/EMLassemblyline")
require("EMLassemblyline")  

# create stub of citations for packages
pkgBibTex <- lapply(c("base", pkgList, RRpackages), citation)

# pkgBibTex <- do.call()

knitr::opts_chunk$set(
   root.dir = params$projectDir,  # from YAML parameter, knitr instead of setwd()
   echo = FALSE,
   comment = " ",
   dev = "svg",
   fig.path = "figures/",
   tidy.opts = list(width.cutoff = 60),
   tidy = TRUE
   )
# if ggplot, update theme to default to centered titles
if ("ggplot2" %in% .packages()) {
   theme_update(plot.title = element_text(hjust = 0.5))
}

setwd(params$projectDir)

# Write YAML parameters to file for consistent reuse across report and data packages
save(params,file="data/temp/reportParameters.RData")

conn <- OpenDatabaseConnection()
```

```{r LoadData, include=FALSE}
# Load datasets for use

if (file.exists(file="data/temp/projectMetadata.RData")) {
  load(file="data/temp/projectMetadata.RData")
} else{
  projectMetadata<-list()
}
```

<hr>
# Background & Introduction
## Significance
Streams and lakes in Great Basin National Park (GRBA) provide habitat for aquatic and terrestrial organisms, contribute to local water supplies, and serve as popular visitor destinations. Aquatic and riparian habitats are important compared to their relatively small land cover because they often host endemic biota and support higher levels of biodiversity. Up to 80% of all vertebrates in the western United States depend on riparian habitats for at least part of their life cycles, and more than half are completely dependent on riparian habitats. Recently, the regionally-endemic Bonneville cutthroat trout has been reintroduced to streams and lakes in the park.

## Threats and Stressors
Climate change poses the main threat to surface water resources in GRBA. Climate change affects precipitation and snowmelt patterns, which control stream flow regime--the amount and variability in discharge--as well as lake hydrology and physical limnology. Flow regime is the key driver of ecosystem processes in streams. In addition, climate change affects water temperatures and thermal regimes, which are also important determinants of biotic communities and ecosystem processes in streams and lakes.

Other potential threats include groundwater pumping in valleys adjacent to GRBA, cultural eutrophication, sedimentation, acid rain, input of contaminants, fishing, and the introduction of non-native species. Cultural eutrophication through the atmospheric deposition of anthropogenic nitrogen and phosphorus may lead to changes in food webs and fisheries and can exacerbate hypoxia events in lakes. Acid rain may decrease pH and alter the nutrient status of lakes. Lakes such as those at GRBA with low natural buffering capacity are particularly vulnerable to the effects of acidification. The combined effects of invasive grasses, past fire suppression, and climate change may result in large and catastrophic fires within the park. Large fires, particularly those burning entire watersheds, could be major disturbances for aquatic biota in streams and lakes.

## Sample Design
The Mojave Desert Network Inventory and Monitoring Program (MOJN I&M) monitors nine perennial streams and six subalpine lakes in GRBA as part of the Streams and Lakes protocol (Figure 1).

All nine streams are visited annually in August to collect water quality channel cross sections, water chemistry samples, and benthic macroinvertebrate samples. In addition, five permanent gaging stations on four of the streams are visited biweekly throughout the summer to calibrate sondes that record continuous water quality during the period of high flow. Four of these permanent gaging stations also have pressure transducers that record continuous stage, which is later converted to discharge through a rating curve.

The lakes are visited annually in September to collect water quality depth profiles, water chemistry samples, water clarity through Secchi depth measurements, and water surface elevations through digital level surveys. These water surface elevations are tied to continuous lake level recorded by pressure transducers.

``` {r figure1, echo = FALSE, fig.align = "left", fig.cap = "**Figure 1.** Streams and lakes monitoring locations, Great Basin National Park."}
include_graphics("figures/grba_stlk_map.jpg")
```

## Monitoring Questions
The goal of this protocol is to gather information that will aid in the assessment, conservation, and restoration of these surface water resources. To achieve this goal, MOJN I&M collects data to address the following monitoring questions.

**Streams:**
1) Are the amount or seasonal patterns of discharge changing over time?
2) What is the status of and what are the trends in water chemistry?
3) What is the status of and what are the trends in benthic macroinvertebrate assemblages? 

**Lakes:**
1) Are water levels or the lake ice-free season changing over time?
2) What is the status of and what are the trends in water chemistry?

# Methods

## Data Collection and Sample Processing Methods (optional)
Field methods used are described in *Mojave Desert Network Inventory and Monitoring Streams and Lakes Protocol: Standard Operating Procedures and Supplementary Materials Version 1.0* (Natural Resource Report NPS/MOJN/NRR—2012/593.1), available here: https://irma.nps.gov/DataStore/Reference/Profile/2190896. Specific methods include SOP 6: Handheld Water Quality Instruments, SOP 7: Sample Handling, Storage, and Shipping, SOP 9: Stream Discharge, SOP 10: Lakes Field Procedures, SOP 11: Continuous Water Quality Sonde, and SOP 12: Stream Chemistry and BMI Sampling.

Water chemistry samples were analyzed by the Oregon State University Cooperative Chemical Analytical Laboratory (CCAL). Their laboratory methods can be found here: http://ccal.oregonstate.edu/sops, and their QA/QC measures can be found here: http://ccal.oregonstate.edu/qaqc.

Benthic macroinvertebrate samples were analyzed by the Utah State University BugLab. Their laboratory methods can be found here: https://www.usu.edu/buglab/SampleProcessing/LaboratoryProcedures/, and their QA/QC measures can be found here: https://www.usu.edu/buglab/SampleProcessing/QualityStandards/.

Summaries of these laboratory methods are also described in *Mojave Desert Network Inventory and Monitoring Streams and Lakes Protocol: Standard Operating Procedures and Supplementary Materials Version 1.0* (Natural Resource Report NPS/MOJN/NRR—2012/593.1), available here: https://irma.nps.gov/DataStore/Reference/Profile/2190896. Specific methods include SOP 13: Laboratory Analysis of BMI and SOP 14: Laboratory Analysis of Water Chemistry.

## Additional Data Sources (optional)
Provide descriptions (with citations) of other data sources used.

- USGS Lehman flow
- NWS Wheeler precip

## Data Processing (required if done)
Summarize process and results of any QC processes done that manipulate, change, or qualify data.

## Code Availability (required)
For all studies using custom code in the generation or processing of datasets, a statement must be included in the Methods section, under the subheading "Code availability", indicating whether and how the code can be accessed, including any restrictions to access. This section should also include information on the versions of any software used, if relevant, and any specific variables or parameters used to generate, test, or process the current dataset. Actual analytical code should be provided in Appendices.

# Data Records (required)
The Data Records section should be used to explain each data record associated
with this work, including the repository where this information is stored, and
to provide an overview of the data files and their formats. Each external data
record should be cited as described below. A data citation should also be placed
in the subsection of the Methods containing the data-collection or analytical
procedure(s) used to derive the corresponding record.

Tables should be used to support the data records, and should clearly indicate
the samples and subjects (study inputs), their provenance, and the experimental
manipulations performed on each (please see example Tables below). They should
also specify the data output resulting from each data-collection or analytical
step, should these form part of the archived record.

## Required Elements for this Section

### Summary of datasets created. Example of stock text to include. ###

Two datasets were generated as a part of this effort (Table 2):

* **`r params$dataPackage1Title`.** Comma-separated text file enumerating the specific taxa considered to have federal conservation status in NPS park units. These data were compiled by the National Park Service Biological Resources Division and were last updated on XX. Available at `r paste0("https://irma.nps.gov/DataStore/Reference/Profile/", params$dataPackage1RefID)`. 

* **`r params$dataPackage2Title`.** Comma-seperated text file enumerating the park-specific specific taxa about which data must be protected from release to the public based on federal conservation status. This dataset includes all taxa identified by the USFWS as well as their child taxa and taxonomic synonyms. Available at `r paste0("https://irma.nps.gov/DataStore/Reference/Profile/",params$dataPackage2RefID)`. 


*Also include a table explaining provenance of datasets*

See Appendix for additional notes and examples.

# Data Quality Evaluation

The data within the data records listed above have been reviewed by staff in the NPS Inventory and Monitoring Division to ensure accuracy, completeness, and consistency with documented data quality standards, as well as for usability and reproducibility. The *`r params$dataPackage2Title `* is suitable for its intended use as of the date of processing (`r Sys.Date()`). For additional information on data quality standards for this protocol, refer to *Mojave Desert Network Inventory and Monitoring Streams and Lakes Protocol: Standard Operating Procedures and Supplementary Materials Version 1.0* (Natural Resource Report NPS/MOJN/NRR—2012/593.1), available here: https://irma.nps.gov/DataStore/Reference/Profile/2190896. Specific methods include SOP 15: Quality Assurance Project Plan (QAPP) and SOP 16: Cumulative Measurement Bias.

## Sites not visited {.tabset}
This is a list of sites that were not visited for annual monitoring during a field season. Stream sampling moved from GRBA_S_BAKR2 (South Fork) to GRBA_S_BAKR3 (Main Stem) in 2011. Brown Lake and Dead Lake were not monitored during the pilot field season in 2009. Dead Lake and Johnson Lake were not monitored in 2013 due to the lapse in federal appropriations.
``` {r no.visits}
no.visits <- qcNoAnnualVisit(conn)

no.visits %>%
  dplyr::select(-c(VisitDate, Park, SiteShort)) %>%
  DT::datatable()
```

## Lake level and benchmark elevations {.tabset}
### Benchmark elevations
These are the mean values and standard deviations of final corrected elevations for each benchmark across all field seasons. Digital level readings began in 2018.
``` {r benchmark.elevations}
benchmark.elevations <- SurveyPointElevation(conn)

benchmark.elevations$ClosureError_ft <- round(benchmark.elevations$ClosureError_ft, 4)
benchmark.elevations$FinalCorrectedElevation_ft <- round(benchmark.elevations$FinalCorrectedElevation_ft, 4)

benchmark.elevations %>%
  dplyr::select(-c(Park, SiteShort, DPL, VisitType)) %>%
  DT::datatable(filter = "top")
```

### String survey heights
These are the mean values and standard deviations of string survey heights for each benchmark for each field season. The height of the benchmark above the water surface was typically measured seven times during the string survey. String surveys ended in 2018 and were replaced by digit level readings during that same field season, which have higher accuracy and precision.
``` {r string.heights}
string.heights <- qcStringSurveyHeights(conn)

string.heights$MeanHeight_ft <- round(string.heights$MeanHeight_ft, 4)
string.heights$StDevHeight_ft <- round(string.heights$StDevHeight_ft, 4)

string.heights %>%
  dplyr::select(-c(Park, SiteShort, VisitType)) %>%
  dplyr::ungroup() %>%
  DT::datatable(filter = "top")
```

### String survey elevations
These are the mean values and standard deviations of string survey lake level elevations for each field season. The reference mark designated Benchmark 1 was typically used to calculate the final lake level at each lake. String surveys ended in 2018 and were replaced by digit level readings during that same field season, which have higher accuracy and precision.
``` {r string.elevations}
string.elevations <- qcStringSurveyElevations(conn)

string.elevations$MeanFinalElevation_ft <- round(string.elevations$MeanFinalElevation_ft, 4)
string.elevations$StDevFinalElevation_ft <- round(string.elevations$StDevFinalElevation_ft, 4)

string.elevations %>%
  dplyr::select(-c(Park, SiteShort, VisitType)) %>%
  DT::datatable(filter = "top")
```

### Benchmark plots
This is a plot of the final corrected elevations for each benchmark for each field season. Consistent upward or downward trends in the elevation of a benchmark may indicate that the benchmark is unstable. An abrupt change in the elevation of a benchmark may indicate disturbance at the location of the benchmark. Only digital level readings are included, which began in 2018.
``` {r benchmark.plots}
benchmark.plots <- PlotBenchmarkElevation(conn)

benchmark.plots
```


## Lake water clarity (Secchi depth) {.tabset}
### Depths exceed lake
This is a list of records where Secchi depth measurements are greater than the lake depth entered during the visit.
``` {r depths.exceed.lake}
depths.exceed.lake <- qcSecchiGTDepth(conn)

depths.exceed.lake %>%
  dplyr::select(-c(Park, SiteShort, VisitType, DPL)) %>%
  DT::datatable(filter = "top")
```

There are `r nrow(depths.exceed.lake)` records with this inconsistency.

### Lake dry, depths exist
This is a list of records where the lake **is** dry and clarity data or Secchi depth measurements exist.
``` {r lake.dry.depths.exist}
lake.dry.depths.exist <- qcLakeDryMeasurementsExist(conn)

lake.dry.depths.exist %>%
  dplyr::select(-c(Park, SiteShort, VisitType, DPL)) %>%
  DT::datatable(filter = "top")
```
There are `r nrow(lake.dry.depths.exist)` records with this inconsistency.

### Lake not dry, no clarity
This is a list of records where the lake **is not** dry and calmness, on bottom, or depth to bottom data **do not** exist.
``` {r lake.not.dry.no.clarity}
lake.not.dry.no.clarity <- qcLakeNotDryMeasurementsMissing(conn)

lake.not.dry.no.clarity %>%
  dplyr::select(-c(Park, SiteShort, VisitType, DPL)) %>%
  DT::datatable(filter = "top")
```
There are `r nrow(lake.not.dry.no.clarity)` records with missing data.

### Lake not dry, no depths
This is a list of records where the Secchi disk **is not** visible on the bottom of the lake and Secchi depth measurements **do not** exist.
``` {r lake.not.dry.no.depths}
lake.not.dry.no.depths <- qcSecchiDepthMissing(conn)

lake.not.dry.no.depths %>%
  dplyr::select(-c(Park, SiteShort, VisitType, DPL)) %>%
  DT::datatable(filter = "top")
```
 There are `r nrow(lake.not.dry.no.depths)` records with missing data.

## Discrete water quality {.tabset}
### Stream sanity check
These are stream water quality values that fall above or below the ranges that we typically see in subalpine lake and stream systems. These data are not necessarily incorrect, but they are outliers that should be evaluated using data quality flags and field notes. Wildly impossible values may be the result of instrument malfunction, improper callibration, or typos during data entry. The following records are included in the list below: temperature values greater than 20 C, pH values greater than 10 and less than 6, specific conductance values greater than 1000 uS/cm, dissolved oxygen percent values greater than 110%, and dissolved oxygen concentration values greater than 12 mg/L.
``` {r wq.stream.sanity}
wq.stream.sanity <- qcStreamWqSanity(conn)

wq.stream.sanity %>%
  dplyr::select(-c(Park, SiteType, VisitType)) %>%
  dplyr::relocate(FieldSeason, .after = VisitDate) %>%
  DT::datatable(filter = "top")
```
There are `r nrow(wq.stream.sanity)` measurements outside of expected ranges.

### Lake sanity check
These are lake water quality values that fall above or below the ranges that we typically see in subalpine lake and stream systems. These data are not necessarily incorrect, but they are outliers that should be evaluated using data quality flags and field notes. Wildly impossible values may be the result of instrument malfunction, improper callibration, or typos during data entry. The following records are included in the list below: temperature values greater than 20 C, pH values greater than 10 and less than 6, specific conductance values greater than 1000 uS/cm, dissolved oxygen percent values greater than 110%, and dissolved oxygen concentration values greater than 12 mg/L.
``` {r wq.lake.sanity}
wq.lake.sanity <- qcLakeWqSanity(conn)

wq.lake.sanity %>%
  dplyr::select(-c(Park, SiteType, VisitType)) %>%
  dplyr::relocate(MeasurementDepth_m, .after = VisitDate) %>%
  dplyr::relocate(FieldSeason, .after = VisitDate) %>%
  DT::datatable(filter = "top")
```
There are `r nrow(wq.lake.sanity)` measurements outside of expected ranges.

### Stream data quality flags
These are stream water quality values that have data quality flags. I = Information: These data do not have any suspected problems, but there may be information regarding the equipment or conditions in which they were collected that could inform their interpretation. W = Warning: These data are suspected to have problems and should only be used after careful assessment of instrument and environmental factors. C = Critical: These data are suspected to have serious problems and are likely unusable.
``` {r wq.stream.flags}
wq.stream.flags <- qcStreamWqFlags(conn)

wq.stream.flags %>%
  dplyr::select(-c(Park, SiteType, VisitType)) %>%
  dplyr::relocate(FieldSeason, .after = VisitDate) %>%
  DT::datatable(filter = "top")
```
There are `r nrow(wq.stream.flags)` flagged measurements.

### Lake data quality flags
These are lake water quality values that have data quality flags. I = Information: These data do not have any suspected problems, but there may be information regarding the equipment or conditions in which they were collected that could inform their interpretation. W = Warning: These data are suspected to have problems and should only be used after careful assessment of instrument and environmental factors. C = Critical: These data are suspected to have serious problems and are likely unusable.
``` {r wq.lake.flags}
wq.lake.flags <- qcLakeWqFlags(conn)

wq.lake.flags %>%
  dplyr::select(-c(Park, SiteType, VisitType)) %>%
  dplyr::relocate(MeasurementDepth_m, .after = VisitDate) %>%
  dplyr::relocate(FieldSeason, .after = VisitDate) %>%
  DT::datatable(filter = "top")
```
There are `r nrow(wq.lake.flags)` flagged measurements.

## Water chemistry {.tabset}
### Data quality flags
These are water chemistry values that have data quality flags. I = Information: These data do not have any suspected problems, but there may be information regarding the equipment or conditions in which they were collected that could inform their interpretation. W = Warning: These data are suspected to have problems and should only be used after careful assessment of instrument and environmental factors. C = Critical: These data are suspected to have serious problems and are likely unusable.
``` {r chem.flags}
chem.flags <- qcChemFlags(conn)

chem.flags %>%
  dplyr::select(-c(SampleFrame)) %>%
  dplyr::relocate(FieldSeason, .after = VisitDate) %>%
  DT::datatable(filter = "top")
```
There are `r nrow(chem.flags)` flagged analytes.

### Lab duplicates
Laboratory duplicates and triplicates are re-analyses of an analyte from the same sample. These duplicates and triplicates can be used to confirm or replace a suspicious initial result. This is a list of the relative percent difference (RPD) values for laboratory duplicates and triplicates. Results that exceed the 30% method quality objective (MQO) threshold are flagged.
``` {r lab.dupes}
lab.dupes <- qcChemLabDupes(conn)

lab.dupes %>%
  dplyr::select(-c(SampleFrame)) %>%
  dplyr::relocate(FieldSeason, .after = VisitDate) %>%
  DT::datatable(filter = "top")
```
There are `r nrow(lab.dupes %>% filter(RPD > 30))` duplicates with RPD values above the 30% MQO threshold.

### Field duplicates
Field duplicates are additional samples collected from the same location at approximately the same time and using the same methods as the primary sample. These duplicates can help to detect inconsistency in collection methods or variability in water chemistry at a location. This is a list of the relative percent difference (RPD) values for field duplicates and triplicates. Results that exceed the 30% method quality objective (MQO) threshold are flagged.
``` {r field.dupes}
field.dupes <- qcChemFieldDupes(conn)

field.dupes %>%
  dplyr::select(-c(SampleFrame)) %>%
  dplyr::relocate(FieldSeason, .after = VisitDate) %>%
  DT::datatable(filter = "top")
```
There are `r nrow(field.dupes %>% filter(RPD > 30))` duplicates with RPD values above the 30% MQO threshold.

### Field blanks
Field blanks are bottles filled with distilled water as opposed to water sampled at the location. They are handled the same way as primary samples in the field and can help to detect and identify sources of contamination during sampling. This is a list of analyte concentrations that exceed the method detection limit (MDL) for that analyte.
``` {r field.blanks}
field.blanks <- qcChemFieldBlanks(conn)

field.blanks %>%
  dplyr::select(-c(SampleFrame)) %>%
  dplyr::relocate(FieldSeason, .after = VisitDate) %>%
  DT::datatable(filter = "top")
```
There are `r nrow(field.blanks)` analytes from blanks with concentrations greater than the MDL for that analyte.

### Dissolved nitrogen
These are the records where the concentration of total dissolved nitrogen (TDN) exceeds total nitrogen (UTN). If the discrepancy falls within precision limits, this indicates that nearly all of the nitrogen is dissolved, and the two concentrations are essentially equal to each other. If the discrepancy falls outside of precision limits, this may indicate contamination of the sample.
``` {r chem.TDN}
chem.TDN <- qcChemTDN(conn)

chem.TDN %>%
  dplyr::select(-c(SampleFrame)) %>%
  dplyr::relocate(FieldSeason, .after = VisitDate) %>%
  DT::datatable(filter = "top")
```
There are `r nrow(chem.TDN %>% filter(TDNvUTN >= 0.02))` records where the concentration of dissolved nitrogen is greater than the total nitrogen concentration and the difference is outside the normal limits of variability.

### Nitrate and nitrite
These are the records where the concentration of nitrate (NO3) and nitrite (NO2) exceeds either total nitrogen (UTN) or total dissolved nitrogen (TDN). If the discrepancy falls within precision limits, this indicates that nearly all of the nitrogen or dissolved nitrogen is in the form of nitrate and nitrite, and the two concentrations are essentially equal to each other. If the discrepancy falls outside of precision limits, this may indicate contamination of the sample.
``` {r chem.NO3NO2}
chem.NO3NO2 <- qcChemNO3NO2(conn)

chem.NO3NO2 %>%
  dplyr::select(-c(SampleFrame)) %>%
  dplyr::relocate(FieldSeason, .after = VisitDate) %>%
  DT::datatable(filter = "top")
```
There are `r nrow(chem.NO3NO2)` records where the concentration of nitrate and nitrate is greater than the concentration of either total nitrogen or total dissolved nitrogen.

### Dissolved phosphorus
These are the records where the concentration of total dissolved phosphorus exceeds total phosphorus. If the discrepancy falls within precision limits, this indicates that nearly all of the phosphorus is dissolved, and the two concentrations are essentially equal to each other. If the discrepancy falls outside of precision limits, this may indicate contamination of the sample.
``` {r chem.TDP}
chem.TDP <- qcChemTDP(conn)

chem.TDP %>%
  dplyr::select(-c(SampleFrame)) %>%
  dplyr::relocate(FieldSeason, .after = VisitDate) %>%
  DT::datatable(filter = "top")
```
There are `r nrow(chem.TDP %>% filter(TDPvUTP >= 0.003))` records where the concentration of dissolved phosphorus is greater than the total phosphorus concentration and the difference is outside the normal limits of variability.

### MDL
Minimum detection level (MDL) is defined by the EPA as the "minimum concentration of a substance that can be measured and reported with 99% confidence that the analyte concentration is greater than zero." Below this concentration, presence of the analyte cannot be confirmed. These are the records where the concentration of a certain analyte was less than or equal to the MDL for that analyte.
``` {r chem.MDL}
lookup <- getMDLLookup()
chem.MDL <- qcChemMDL(conn)

chem.MDL %>%
  dplyr::select(-c(SampleFrame)) %>%
  dplyr::relocate(FieldSeason, .after = VisitDate) %>%
  DT::datatable(filter = "top")
```
There are `r nrow(chem.MDL)` analytes with concentrations less than or equal to the MDL.

### ML
Minimum level of quantification (ML) is defined as the "concentration at which the analytical system gives a recognizable signal and acceptable calibration point for the analyte." Below this concentration, the analyte may be detected (if greater than the MDL) but not measured at a known level of confidence. These are the records where the concentration of a certain analyte was less than or equal to the ML for that analyte.
``` {r chem.ML}
lookup <- getMDLLookup()
chem.ML <- qcChemML(conn)

chem.ML %>%
  dplyr::select(-c(SampleFrame)) %>%
  dplyr::relocate(FieldSeason, .after = VisitDate) %>%
  DT::datatable(filter = "top")
```
There are `r nrow(chem.ML)` analytes with concentrations less than or equal to the ML.

## Benthic macroinvertebrates {.tabset}
### Discrepancies
These are records with discrepancies between taxa group count (number of taxa within a functional group) and taxa group abundance (abundance of individuals within those taxa). Discrepancies include situations where the taxa count is non-zero but abundance is zero and where abundance is non-zero and taxa count is zero. These situations may arise from the standardization of richness-based (taxa group count) metrics to Operational Taxonomic Units (OTUs), while density-based (taxa group abundance) metrics are based on the raw taxa list.
``` {r bmi.discrepancies}
bmi.discrepancies <- qcBMIDiscrepancies(conn)

bmi.discrepancies %>%
  dplyr::select(-c(Park, SiteShort, VisitType)) %>%
  dplyr::relocate(FieldSeason, .after = VisitDate) %>%
  DT::datatable(filter = "top")
```
There are `r nrow(bmi.discrepancies)` metrics with discrepancies between taxa group count and taxa group abundance.

# Usage Notes (required)
The Usage Notes should contain brief instructions to assist other researchers
with reuse of the data. This may include discussion of software packages that
are suitable for analysing the assay data files, suggested downstream processing
steps (e.g. normalization, etc.), or tips for integrating or comparing the data
records with other datasets. Authors are encouraged to provide code, programs or
data-processing workflows if they may help others understand or use the data.

For studies involving privacy or safety controls on public access to the data,
this section should describe in detail these controls, including how authors can
apply to access the data, what criteria will be used to determine who may access
the data, and any limitations on data use.

# Acknowledgements
The authors would like to acknowledge Geoff Moret and Chris Caudill, who designed and wrote the Streams and Lakes protocol and set in motion over a decade of surface water resource monitoring in GRBA.

# References (required)
Bibliographic information for any works cited in the above sections, using the
standard *NPS NR Publication Series* referencing style.

In line with emerging [industry-wide standards for data
citation](https://www.nature.com/articles/sdata2018259), references to all
datasets described or used in the manuscript should be cited in the text and
listed in the ‘References’ section in the same manner as a conventional
literature reference.

ITIS. 2020. Integrated Taxonomic Information System. Available at: https://www.itis.gov/ (accessed 29 January 2020).

National Park Service (NPS). 2010. Draft Reference Manual RM 66B: Handling Protected Information. National Park Service, Washington, DC. (Available at https://irma.nps.gov/DataStore/Reference/Profile/2224216)

National Park Service (NPS). 2019. NPSpecies - the National Park Service biodiversity database. Available at: https://irma.nps.gov/NPSpecies/ (accessed xx).

Office of Management and Budget (OMB). 2013. Open Data Policy-Managing Information as an Asset. Office of Management and Budget. (Available at https://digital.gov/open-data-policy-m-13-13/)

U.S. Fish & Wildlife Service (USFWS). 2019. ECOS Environmental Conservation Online System. Available at: https://ecos.fws.gov/ecp/ (accessed xx).

U.S. Government Printing Office (GPO). 2013. Making Open and Machine Readable the New Default for Government Information. Executive Order 13642.(Available at https://www.govinfo.gov/content/pkg/CFR-2014-title3-vol1/pdf/CFR-2014-title3-vol1-eo13642.pdf)


\pagebreak

# Appendix C. Code Listing
```{r Listing, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```

\pagebreak

# Appendix D. Session and Version Information
```{r session-info, echo=FALSE, cache=FALSE}
sessionInfo()
Sys.time()
```

# Additional Notes (this should not be included in reports...)

## Figures

Figure images should be included in-text near the initial point of reference.

Figure captions begin with a brief title sentence summarizing the purpose of the figure as a whole, and continue with a short description of what is shown in each panel and an explanation of any symbols used. Legends must total no more than 350 words, and may contain literature references. The first sentence of the legend will be used as the title for the figure. It (the first sentence) should contain no references of any kind, including to specific figure panels, bibliographic citations or references to other figures or panels.

## Tables

Authors are encouraged to provide one or more tables that provide basic
information on the main ‘inputs’ to the study (e.g. samples, participants, or
information sources) and the main data outputs of the study; also see the
additional information on providing metadata on page 6. Tables in the manuscript
should generally not be used to present primary data (i.e. measurements). Tables
containing primary data should be submitted to an appropriate data repository.

Authors may provide tables within text near the initial citation or as an
appendix. Legends, where needed, should be included in the Word document.
Generally, a Data Publication Report should have fewer than ten tables, but more
may be allowed when needed.

### Example Data Record Summary Tables
Here, we provide four generic ‘Table 1’ examples, including two experimental
study examples, one observational study example, and an example for an
aggregated dataset of the type that may result from a meta-analysis. 

```{r Table1, echo=FALSE}
T1Subjects<-c("Mouse1","Mouse2","Mousen")
T1Protocol1<-c("Drug treatment","Drug treatment","Drug treatment")
T1Protocol2<-c("Liver dissection","Liver dissection","Liver dissection")
T1Protocol3<-c("RNA extraction","RNA extraction","RNA extraction")
T1Protocol4<-c("RNA-Seq","RNA-Seq","RNA-Seq")
T1Data<-c("GEOXXXXX","GEOXXXXX","GEOXXXXX")
Table1<-data.frame(T1Subjects,T1Protocol1,T1Protocol2,T1Protocol3,T1Protocol4,T1Data)

kable(Table1, 
      col.names=c("Subjects","Protocol 1","Protocol 2","Protocol 3","Protocol 4","Data"),
      caption="**Table 1.** Experimental study example Data Records table.") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),full_width=F) %>%
  footnote(general="This table was generated using the kableExtra package.")

```

```{r Table2, echo=FALSE}
Source<-c("CellCulture1","CellCulture1","CellCulture1","CellCulture1","CellCulture1","CellCulture1")
Protocol1<-c("Drug treatment","Drug treatment","Drug treatment","Drug treatment","Drug treatment","Drug treatment")
Protocol2<-c("RNA extraction","RNA extraction","RNA extraction","RNA extraction","RNA extraction","RNA extraction")
Samples<-c("TechnicalRep1a","TechnicalRep2a","TechnicalRep3a","TechnicalRep1b","TechnicalRep2b","TechnicalRep3b")
Protocol3<-c("Microarray hybridization","Microarray hybridization","Microarray hybridization","Microarray hybridization","Microarray hybridization","Microarray hybridization")
Data<-c("GEOXXXXX","GEOXXXXX","GEOXXXXX","GEOXXXXX","GEOXXXXX","GEOXXXXX")
Table<-data.frame(Source,Protocol1,Protocol2,Samples,Protocol3,Data)

kable(Table, 
      col.names=c("Subjects","Protocol 1","Protocol 2","Samples","Protocol 3","Data"),
      caption="**Table 2.** Experimental study with replicates Data Records table.") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),full_width=F)


```

```{r Table3, echo=FALSE}
Sample<-c("Body of water 1","Body of water 2","Body of water n")
geoloc<-c("location name","location name","location name")
geopos<-c("latitude, longitude, altitude","latitude, longitude, altitude","latitude, longitude, altitude")
protocol<-c("Measurement of surface temperature","Measurement of surface temperature","Measurement of surface temperature")
data<-c("dataFile1","dataFile2","dataFile3")
Table<-data.frame(Sample,geoloc,geopos,protocol,data)

kable(Table, 
      col.names=c("Sample","Geographical Location","Geoposition","Protocol","Data"),
      caption="**Table 3.** Observational study example Data Records table.") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),full_width=F)


```

```{r Table4, echo=FALSE}
c1<-c("Database URL 1","Database URL 1","Database URL 2")
c2<-c("Dataset 1","Dataset 2","Dataset n")
c3<-c("Number of samples in the dataset","Number of samples in the dataset","Number of samples in the dataset")
c4<-c("Range of measurements reported in the dataset","Range of measurements reported in the dataset","Range of measurements reported in the dataset")
c5<-c("Data assimilation procedure","Data assimilation procedure","Data assimilation procedure")
c6<-c("Method to generate output data","Method to generate output data","Method to generate output data")
c7<-c("dataFile1","dataFile1","dataFile2")
Table<-data.frame(c1,c2,c3,c4,c5,c6,c7)

kable(Table, 
      col.names=c("Source","Sample","Sample Number","Temporal Range","Protocol 1","Protocol2","Data"),
      caption="**Table 4.** Data aggregation study example Data Records table.") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),full_width=F)


```

